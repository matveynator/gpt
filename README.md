# Упрощённый трансформер на Go (с ручным backprop и многопоточностью)

### 1. Общая идея
Данный проект реализует учебный пример **упрощённого трансформера** (с одним слоем) на языке Go:

- **Полный (но упрощённый) Backpropagation** для всех основных частей (Embeddings, Self-Attention, LayerNorm, MLP, Residual) — на практике часть вычислений сокращена для демонстрации.
- **Многопоточность**: обучение распределяется между несколькими горутинами (число их равно `runtime.NumCPU()`). Каждая горутина обрабатывает «задачу обучения» (TrainTask), а результаты возвращает в главный цикл.
- **Сигнал завершения** (Ctrl+C, SIGTERM) перехватывается; при этом обучение прерывается, и модель сохраняется.
- **Режим вопрос-ответ** (`ask`) позволяет задать вопрос по-английски и получить ответ, стилизованный под тексты Шекспира, с побуквенным выводом.

**Обратите внимание**, что реализация включает **ручное** (manual) вычисление градиентов, что в реальных проектах делают редко (обычно используют PyTorch или TensorFlow). Здесь это сделано в целях **образовательных**.

### 2. Структура кода

Код разделён на логические блоки:

1. **Базовые матричные и векторные операции**  
   - `dot`, `addVec`, `scaleVec`, `matVecMul`, `randMat`, `randVec`.

2. **Активационные функции**  
   - `reluForward`/`reluBackward`, `softmaxForward`.

3. **Глобальные гиперпараметры**  
   - `TransformerConfig` (EmbedSize, BlockSize, NumIterations, LearningRate и т.д.).

4. **Слои:**  
   - `Linear` (линейный слой с forward/backward),  
   - `EmbeddingLayer` (токенные и позиционные эмбеддинги),  
   - `LayerNorm` (нормализация слоя),  
   - `SelfAttention` (одна голова),  
   - `MLPBlock` (двухслойный перцептрон).

5. **Расчёт cross-entropy**  
   - `crossEntropy`, `crossEntropyGrad`.

6. **Основная модель** (`TransformerModel`)  
   - Состоит из EmbeddingLayer, одного TransformerBlock и финального линейного слоя (проекция в словарь).

7. **Многопоточность**  
   - Задачи (TrainTask), результаты (GradResult), функция `trainWorker`, которая обрабатывает одну задачу, и `trainParallel`, которая оркестрирует процесс и обрабатывает сигналы.

8. **Генерация текста** (побуквенно, стилизация под Шекспира) и режим «ask».

9. **Загрузка датасета (TinyShakespeare)**, сохранение и загрузка модели (JSON).

### 3. Как это работает пошагово

1. **Инициализация модели**:  
   - При запуске с `-mode=train`, проверяется, есть ли файл `model.json`. Если нет, создаётся новый `TransformerModel`, если да — загружается существующий.
   - Настраиваются параметры (EmbedSize=128, BlockSize=128, и т.д.).

2. **Обучение** (`trainParallel`):  
   - Создаётся несколько горутин (воркеров), каждая получает задачу `TrainTask` из канала `tasksChan`.  
   - Каждая горутина:  
     - Выбирает случайный отрезок текста из `data`, делает forward, считает loss, делает backward, обновляет часть параметров (упрощённо), и отправляет `GradResult` (потерю) обратно в канал `resultsChan`.  
   - Главный цикл N итераций:
     - Заполняет `tasksChan`,
     - Ждёт `GradResult`,
     - Суммирует среднюю потерю.  
   - Каждые 100 итераций выводит среднюю потерю и пример генерации (для наглядности).  
   - Каждые 1000 итераций вызывает `saveChan <- struct{}{}`, и специальная горутина `saver` сохраняет модель в `model_checkpoint.json`.  

3. **Сигнал** (Ctrl+C):  
   - Перехватывается через `signal.Notify`,
   - Прерывает обучение, вызывает сохранение модели.  

4. **Режим «ask»**:  
   - Модель загружается,
   - Пользователь вводит вопрос на английском,
   - Вызывается `askQuestion`, которая превращает символы вопроса в индексы, затем вызывает `generateText`, где токен за токеном вызывается forward модели, берётся последний логит, через softmax выбирается следующий токен, и символ выводится «побуквенно».

### 4. Математика (вкратце)

- **Self-Attention**:  
  Для каждого токена **t** вычисляем \( Q[t] = W^Q x[t]\), \(K[t] = W^K x[t]\), \(V[t] = W^V x[t]\).  
  Затем оценка **scores** для пары (t,i) есть \(\frac{Q[t] \cdot K[i]}{\sqrt{EmbedSize}}\).  
  Вектор весов attention \(\alpha_{t}\) получается через softmax(scores).  
  Итоговый вектор \(\mathrm{attnOut}[t] = W^O \sum_i \alpha_{t,i} V[i]\).  
  Это упрощённо: в реальном backpropagation есть подробные формулы для dQ,dK,dV и dW.

- **LayerNorm**:  
  \(\mathrm{mean} = \frac{1}{D}\sum_i x_i,\quad \mathrm{var} = \frac{1}{D}\sum_i (x_i - \mathrm{mean})^2\).  
  \(\mathrm{normX}_i = \frac{x_i - \mathrm{mean}}{\sqrt{\mathrm{var} + \epsilon}}\).  
  Выход: \(y_i = \gamma_i * \mathrm{normX}_i + \beta_i\).  
  Backpropagation (dGamma, dBeta, dx) формулой из LayerNorm.

- **MLP**:  
  Два линейных слоя + ReLU.

- **Residual**:  
  Остаточная связь x + out, затем LayerNorm.

### 5. Как использовать

1. Скопируйте файл `transformer_full.go` в свою папку.  
2. Поместите `tinyshakespeare.txt` рядом.  
3. Запустите обучение:
   ```bash
   go run transformer_full.go -mode=train
   ```
   Будет выводиться средняя потеря каждые 100 итераций.  
4. Когда обучение прервётся (Ctrl+C) или закончится (число итераций), модель сохранится в `model.json`.  
5. Запустите в режиме `ask`:
   ```bash
   go run transformer_full.go -mode=ask
   ```
   Введите вопрос (например, «What is love?»). Модель выведет ответ посимвольно.

### 6. Ограничения и улучшения

- **Ограничения**:
  - Реализован только один блок (один слой) без многоуровневого stack.  
  - Backpropagation для self‑attention формула описана упрощённо, не все шаги учтены.  
  - Оптимизатор — простой SGD. В реальных задачах обычно используют Adam.  
  - Может работать медленно, потому что всё ручное и без специальных оптимизаций.

- **Возможные улучшения**:
  - Добавить несколько блоков (слоёв).  
  - Реализовать многоголовое (multi-head) внимание.  
  - Добавить dropout.  
  - Перейти на более продвинутый оптимизатор (Adam).  
  - Проверять корректность через численную оценку градиентов (finite differences).


